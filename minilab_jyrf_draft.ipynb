{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category \tAvailable \tRequirements\n",
    "Total Points \t100\t\n",
    "Create Models\t50\tCreate a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. \n",
    "Model Advantages\t10\tDiscuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "Interpret Feature Importance\t30\tUse the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "Interpret Support Vectors\t10\tLook at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You must include all of your data wrangling code, if you want to receive full credit.  Otherwise, your work is not reproducible.  This is true for all submissions. You are the only group who left this out. I need to see what you started with and all transformations. The dataset must still meet the minimum size after all transformations.  I will not accept future labs without this code included.  \n",
    "\n",
    "2. I have stated multiple times that the minimum dataset size is 15 columns x 30,000 rows = 450,000 elements. My feedback clearly states that what you submitted is currently at 16 * 9731 = 155,696 elements. As I said in the feedback: \"In future labs, you will need to increase the number of fields you use for modeling to achieve at least 15 * 30,000 = 450,000 elements.\". \n",
    "\n",
    "3. Unfortunately for Lab 1, it really makes no difference what your yearly tables may look like at this point, because they were not included in your submission? \n",
    "\n",
    "4. I see above that the dataset you produced for each year has many more fields / columns / features that you could have included to meet the minimum size requirements?  The starting point should have likely been that entire dataset.  I am also slightly concerned about your statement that \"merging is a simple inner join activity\" for a number of reasons. The dataset you started with does not appear to be joined at all.  There is a year column in the dataset.  This gives me the impression that you actually unioned the data by year (which is what I would have expected). However, I really have no clue what you did, because you chose not to share that with me.\n",
    "\n",
    "Thoughts on inner joining this data:\n",
    "A number of public school campuses open and close each school year.  If you are joining by unit_code / agency_code, your analysis will only include school campuses open during the entire duration of the data.\n",
    "Any classification model you create in a \"joined\" dataset will likely have duplicated columns for each year.  This means that the feature importances for the model will be dominated by prior years.  For example, if you build a model to predict graduation rates, the model will use all prior year's graduation rates for the prediction.  It will also be very accurate.  However, I am not sure what benefit such a model would provide to the public school system?\n",
    "Typically, you would not include any graduation rate features in your graduation rate model training data at all. (we can talk about this more, if needed).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniLab SVM and Logistic Regression\n",
    "#### 06/14/2020\n",
    "#### Yang Zhang, Reannan McDaniel, Jonathon Roach, Fred Poon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### One-Hot Encoding\n",
    "### Feature Selection\n",
    "### Training/Testing Splitting\n",
    "### Solving Data Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "### SVM Work\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
